import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import re

# æˆ‘ä»¬è¦æŠ“å–æ•°æ®çš„æœºå™¨äººåŠå…¶ç»´åŸºç™¾ç§‘é¡µé¢URL
ROBOT_WIKI_URLS = {
    'spot': 'https://en.wikipedia.org/wiki/Spot_(robot)',
    # ä»¥åå¯ä»¥ç»§ç»­åœ¨è¿™é‡Œæ·»åŠ ï¼Œæ¯”å¦‚ 'atlas': '...'
}

def clean_text(text):
    """ä¸€ä¸ªæ›´å¼ºå¤§çš„æ–‡æœ¬æ¸…ç†å‡½æ•°ï¼Œå»é™¤å¼•ç”¨æ ‡è®°å¦‚[1]å’Œå¤šä½™çš„æ¢è¡Œ"""
    # å»é™¤å¼•ç”¨æ ‡è®°ï¼Œä¾‹å¦‚ [1], [2], [a], etc.
    text = re.sub(r'\[.*?\]', '', text)
    # å°†æ¢è¡Œç¬¦å’Œå¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def get_infobox_data(soup):
    """ä»ç»´åŸºç™¾ç§‘çš„infoboxä¸­æå–å…³é”®ä¿¡æ¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    infobox = soup.find('table', {'class': 'infobox'})
    if not infobox:
        return {}

    data = {}
    rows = infobox.find_all('tr')
    for row in rows:
        header = row.find('th')
        value_cell = row.find('td')
        
        if header and value_cell:
            # å°†keyè½¬æ¢ä¸ºå°å†™ï¼Œä¾¿äºç»Ÿä¸€å¤„ç†
            key = clean_text(header.text).lower()
            # è·å–valueï¼Œå¹¶åªå–ç¬¬ä¸€ä¸ª<br>ä¹‹å‰çš„å†…å®¹ï¼ˆå¤„ç†å¤šè¡Œæ•°æ®ï¼‰
            value_text = value_cell.get_text(separator='|', strip=True).split('|')[0]
            val = clean_text(value_text)
            data[key] = val
            
    return data

def scrape_robot_data(name, url):
    """æŠ“å–å•ä¸ªæœºå™¨äººçš„æ•°æ®å¹¶æ ¼å¼åŒ–ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    try:
        response = requests.get(url, headers={'User-Agent': 'Cool-Robot-App-Scraper/1.0'})
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        infobox_data = get_infobox_data(soup)
        
        # å°†æŠ“å–åˆ°çš„æ•°æ®æ˜ å°„åˆ°æˆ‘ä»¬çš„æ•°æ®ç»“æ„ä¸­
        # è¿™é‡Œæˆ‘ä»¬ç”¨ .get() æ–¹æ³•ï¼Œå¹¶æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„keyï¼Œæé«˜é²æ£’æ€§
        robot_data = {
            'name': clean_text(soup.find('h1', {'id': 'firstHeading'}).text),
            'manufacturer': infobox_data.get('manufacturer', 'N/A'),
            'type': infobox_data.get('type', 'N/A'),
            'specs': {
                'Weight': infobox_data.get('weight', infobox_data.get('mass', 'N/A')),
                'Payload': infobox_data.get('payload', 'N/A'),
                'Speed': infobox_data.get('speed', 'N/A'),
            },
            # æ¨¡å—å’Œä¾›åº”å•†ä¿¡æ¯ä»ç„¶ä½¿ç”¨å ä½ç¬¦ï¼Œå› ä¸ºè¿™éƒ¨åˆ†ä¿¡æ¯å¾ˆéš¾ä»é€šç”¨é¡µé¢æŠ“å–
            'modules': {
                'Perception': {'components': ['Cameras', 'IMU', 'Sensors'], 'suppliers': ['Various']},
                'Locomotion': {'components': ['Actuators', 'Legs', 'Motors'], 'suppliers': ['Various']},
            }
        }
        print(f"âœ… Successfully scraped data for: {name}")
        return robot_data
    except Exception as e:
        print(f"âŒ Failed to scrape data for {name}. Error: {e}")
        return None

if __name__ == '__main__':
    all_robots_data = {}
    for name, url in ROBOT_WIKI_URLS.items():
        data = scrape_robot_data(name, url)
        if data:
            all_robots_data[name] = data
            
    # å‡†å¤‡æœ€ç»ˆçš„JSONè¾“å‡º
    output = {
        'last_updated': datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC'),
        'robots': all_robots_data
    }
    
    # å°†ç»“æœå†™å…¥æˆ‘ä»¬çš„æ•°æ®æ–‡ä»¶
    with open('data/robots.json', 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
        
    print("ğŸš€ Scraping complete. `data/robots.json` has been updated.")
