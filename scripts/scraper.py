import requests
import json
import os
from datetime import datetime
from bs4 import BeautifulSoup
import google.generativeai as genai

# ä»ç¯å¢ƒå˜é‡ä¸­è·å–API Key
API_KEY = os.getenv("GEMINI_API_KEY")

if not API_KEY:
    raise ValueError("Gemini API Key not found. Please set the GEMINI_API_KEY secret.")

# é…ç½®Google Generative AI SDK
genai.configure(api_key=API_KEY)

# ç›®æ ‡æœºå™¨äººåˆ—è¡¨ä¿æŒä¸å˜
ROBOT_WIKI_URLS = {
    'spot': 'https://en.wikipedia.org/wiki/Spot_(robot)',
    'atlas': 'https://en.wikipedia.org/wiki/Atlas_(robot)',
    'digit': 'https://en.wikipedia.org/wiki/Digit_(robot)'
}

def build_prompt(page_content):
    """æ„å»ºä¸€ä¸ªé«˜è´¨é‡çš„Promptï¼ŒæŒ‡å¯¼Geminiå®Œæˆä»»åŠ¡"""
    
    desired_json_structure = """
    {
      "name": "Robot's full name",
      "manufacturer": "The company that created the robot",
      "type": "Type of robot (e.g., Quadruped, Humanoid)",
      "specs": {
        "Weight": "Weight of the robot (e.g., 75 kg)",
        "Payload": "Payload capacity (e.g., 25 kg)",
        "Speed": "Maximum speed (e.g., 1.5 m/s)"
      },
      "modules": {
        "Perception": { "components": ["List of sensors like Cameras, LiDAR, IMU"], "suppliers": ["List of potential suppliers"] },
        "Locomotion": { "components": ["List of components like Actuators, Hydraulic systems"], "suppliers": ["List of potential suppliers"] }
      }
    }
    """

    # Gemini å¯¹æŒ‡ä»¤çš„ç†è§£èƒ½åŠ›å¾ˆå¼ºï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è¦æ±‚å®ƒè¾“å‡ºJSON
    prompt = f"""
    Analyze the following text from a Wikipedia page about a robot.
    Your task is to extract key information and provide the output ONLY in a valid JSON format.
    The JSON object must strictly adhere to the structure shown below.
    If a piece of information is not available in the text, use "N/A" as the value.
    Do not include any introductory text, closing remarks, or markdown formatting like ```json.
    
    ### Desired JSON Structure:
    {desired_json_structure}

    ### Page Content to Analyze:
    ---
    {page_content[:20000]} 
    ---

    ### Extracted JSON Data:
    """
    return prompt

def scrape_with_gemini(name, url):
    """ä½¿ç”¨Geminiæ¨¡å‹æŠ“å–å•ä¸ªæœºå™¨äººçš„æ•°æ®"""
    print(f"âœ¨ Attempting to scrape '{name}' with Gemini...")
    try:
        # 1. è·å–ç½‘é¡µçº¯æ–‡æœ¬
        response = requests.get(url, headers={'User-Agent': 'Robot-Genesis-Gemini-Scraper/1.0'})
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        main_content = soup.find(id='mw-content-text')
        page_text = main_content.get_text(separator=' ', strip=True) if main_content else soup.get_text(separator=' ', strip=True)

        # 2. æ„å»ºPrompt
        prompt = build_prompt(page_text)
        
        # 3. åˆå§‹åŒ–å¹¶è°ƒç”¨Geminiæ¨¡å‹
        model = genai.GenerativeModel('gemini-pro')
        # æ·»åŠ å®‰å…¨è®¾ç½®ï¼Œé™ä½å› å®‰å…¨ç­–ç•¥è¢«é˜»æ–­çš„æ¦‚ç‡
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]
        
        # è®©æ¨¡å‹ç”Ÿæˆå†…å®¹
        response = model.generate_content(prompt, safety_settings=safety_settings)
        
        # 4. è§£æGeminiè¿”å›çš„ç»“æœ
        # response.text ç›´æ¥å°±æ˜¯æ¨¡å‹ç”Ÿæˆçš„å­—ç¬¦ä¸²
        json_text = response.text
        
        # éªŒè¯å¹¶åŠ è½½JSON
        robot_data = json.loads(json_text)
        
        print(f"âœ… Successfully parsed Gemini response for: {name}")
        return robot_data

    except Exception as e:
        # æ•è·å¹¶æ‰“å°æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        if 'response' in locals() and hasattr(response, 'prompt_feedback'):
             print(f"Prompt Feedback: {response.prompt_feedback}")
        print(f"âŒ Failed to scrape data for '{name}' with Gemini. Error: {e}")
        return None

if __name__ == '__main__':
    all_robots_data = {}
    for name, url in ROBOT_WIKI_URLS.items():
        data = scrape_with_gemini(name, url)
        if data:
            all_robots_data[name] = data
            
    output = {
        'last_updated': datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC'),
        'robots': all_robots_data
    }
    
    with open('data/robots.json', 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
        
    print("ğŸš€ Gemini-driven scraping complete. `data/robots.json` has been updated.")
