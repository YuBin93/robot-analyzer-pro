import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime

# æˆ‘ä»¬è¦æŠ“å–æ•°æ®çš„æœºå™¨äººåŠå…¶ç»´åŸºç™¾ç§‘é¡µé¢URL
# ä»¥åæƒ³æ·»åŠ æ–°æœºå™¨äººï¼Œåªéœ€è¦åœ¨è¿™é‡ŒåŠ ä¸Šæ–°æ¡ç›®å³å¯
ROBOT_WIKI_URLS = {
    'spot': 'https://en.wikipedia.org/wiki/Spot_(robot)',
    # 'atlas': 'https://en.wikipedia.org/wiki/Atlas_(robot)', # å¯ä»¥å–æ¶ˆæ³¨é‡Šæ¥æ·»åŠ Atlas
}

def get_infobox_data(soup):
    """ä»ç»´åŸºç™¾ç§‘çš„infoboxä¸­æå–å…³é”®ä¿¡æ¯"""
    infobox = soup.find('table', {'class': 'infobox'})
    if not infobox:
        return {}

    data = {}
    rows = infobox.find_all('tr')
    for row in rows:
        header = row.find('th')
        value = row.find('td')
        if header and value:
            # æ¸…ç†æ–‡æœ¬
            key = header.text.strip().lower()
            val = value.text.strip()
            data[key] = val
    return data

def scrape_robot_data(name, url):
    """æŠ“å–å•ä¸ªæœºå™¨äººçš„æ•°æ®å¹¶æ ¼å¼åŒ–"""
    try:
        response = requests.get(url, headers={'User-Agent': 'Cool-Robot-App-Scraper/1.0'})
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        infobox_data = get_infobox_data(soup)
        
        # å°†æŠ“å–åˆ°çš„æ•°æ®æ˜ å°„åˆ°æˆ‘ä»¬çš„æ•°æ®ç»“æ„ä¸­
        # è¿™æ˜¯æœ€éœ€è¦å®šåˆ¶çš„éƒ¨åˆ†ï¼Œå› ä¸ºæ¯ä¸ªé¡µé¢çš„å­—æ®µåéƒ½ä¸åŒ
        robot_data = {
            'name': soup.find('h1', {'id': 'firstHeading'}).text.strip(),
            'manufacturer': infobox_data.get('manufacturer', 'N/A'),
            'type': infobox_data.get('type', 'Robot'),
            'specs': {
                'Weight': infobox_data.get('weight', 'N/A'),
                'Payload': infobox_data.get('payload', 'N/A'),
                'Speed': infobox_data.get('speed', 'N/A'),
            },
            # æ¨¡å—å’Œä¾›åº”å•†ä¿¡æ¯é€šå¸¸å¾ˆéš¾ä»ç»´åŸºç™¾ç§‘æŠ“å–ï¼Œè¿™é‡Œç”¨å ä½ç¬¦
            'modules': {
                'Perception': {'components': ['Cameras', 'IMU'], 'suppliers': ['Various']},
                'Locomotion': {'components': ['Actuators', 'Legs'], 'suppliers': ['Various']},
            }
        }
        print(f"âœ… Successfully scraped data for: {name}")
        return robot_data
    except Exception as e:
        print(f"âŒ Failed to scrape data for {name}. Error: {e}")
        return None

if __name__ == '__main__':
    all_robots_data = {}
    for name, url in ROBOT_WIKI_URLS.items():
        data = scrape_robot_data(name, url)
        if data:
            all_robots_data[name] = data
            
    # å‡†å¤‡æœ€ç»ˆçš„JSONè¾“å‡º
    output = {
        'last_updated': datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC'),
        'robots': all_robots_data
    }
    
    # å°†ç»“æœå†™å…¥æˆ‘ä»¬çš„æ•°æ®æ–‡ä»¶
    with open('data/robots.json', 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
        
    print("ğŸš€ Scraping complete. `data/robots.json` has been updated.")
